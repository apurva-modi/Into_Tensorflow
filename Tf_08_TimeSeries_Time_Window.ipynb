{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time_Window.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owGfxhNzIcYy",
        "colab_type": "text"
      },
      "source": [
        "## Time Window \n",
        "Creating a function with time series as an input using the tensorflow dataset that we can load and use for our Tensorflow model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4S070GfHZLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # if in colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW4J05I2PAe0",
        "colab_type": "text"
      },
      "source": [
        "We will first train a model to forecast the next step given the pervious 20 steps, therefore we need to create a dataset of 20-steps windows for training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOG-XwCmIfAB",
        "colab_type": "code",
        "outputId": "3d54a9e2-bae6-4e25-c94f-8404e76d2dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# We are creating a dataset that contains the intergers from 0 to 9 \n",
        "dataset = tf.data.Dataset.range(10) \n",
        "for val in dataset:\n",
        "  print(val.numpy()) # val gives all the tensors and using val.numpy() gives us the actual value."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD4FLTKUPcNy",
        "colab_type": "code",
        "outputId": "310278d1-fcb4-492e-d5b2-78f965ef606c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "# calling the window method which requires window size \n",
        "dataset= dataset.window(5, shift=1) # specifing shift = 1 which means each window will be shifted 1 time comapred to the previous window\n",
        "# each window in the dataset is actually a dataset \n",
        "# if we iterate over window then, we can actually iterate over through each element(value) in the window.\n",
        "for window_dataset in dataset:\n",
        "  for vals in window_dataset:\n",
        "    print(vals.numpy(),end = \" \")\n",
        "  print( )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 3 4 \n",
            "1 2 3 4 5 \n",
            "2 3 4 5 6 \n",
            "3 4 5 6 7 \n",
            "4 5 6 7 8 \n",
            "5 6 7 8 9 \n",
            "6 7 8 9 \n",
            "7 8 9 \n",
            "8 9 \n",
            "9 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rFFh15WTTge",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "44e31dfc-b851-4806-9e74-301f96544005"
      },
      "source": [
        "# specifying the drop reminder as true so that we get all the window of same size. \n",
        "#\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "# calling the window method which requires window size \n",
        "dataset= dataset.window(5, shift=1, drop_remainder= True) # specifing shift = 1 which means each window will be shifted 1 time comapred to the previous window\n",
        "# each window in the dataset is actually a dataset \n",
        "# if we iterate over window then, we can actually iterate over through each element(value) in the window.\n",
        "for window_dataset in dataset:\n",
        "  for vals in window_dataset:\n",
        "    print(vals.numpy(),end = \" \")\n",
        "  print( )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 3 4 \n",
            "1 2 3 4 5 \n",
            "2 3 4 5 6 \n",
            "3 4 5 6 7 \n",
            "4 5 6 7 8 \n",
            "5 6 7 8 9 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsKv0-3ok33q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9943f963-c6f9-4363-a673-9dabef449ed3"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset= dataset.window(5, shift=1, drop_remainder= True)\n",
        "# creating tensor of size(5) on the window using the `flat_method` method\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "for window in dataset:\n",
        "  print(window.numpy())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4]\n",
            "[1 2 3 4 5]\n",
            "[2 3 4 5 6]\n",
            "[3 4 5 6 7]\n",
            "[4 5 6 7 8]\n",
            "[5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41jTK5h2lyPt",
        "colab_type": "text"
      },
      "source": [
        "> Consider the **first 4** elements in the window to be `input features` and the **last one** as the `output label`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzp18WpSlelT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6dd755cd-a341-4f84-9b02-99fec30b4fba"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset= dataset.window(5, shift=1, drop_remainder= True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "for x,y in dataset:\n",
        "  print(x.numpy(), y.numpy())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3] [4]\n",
            "[1 2 3 4] [5]\n",
            "[2 3 4 5] [6]\n",
            "[3 4 5 6] [7]\n",
            "[4 5 6 7] [8]\n",
            "[5 6 7 8] [9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEK-5RFxzzqK",
        "colab_type": "text"
      },
      "source": [
        "> We need to make sure that the instance of the dataset is shuffled. \n",
        "this is to ensure that they are independent and identically distributed or IID, which is necessary especially if you're using gradient descent, which is usually the case. \n",
        "To do this we will call the shuffle method and specify the buffer_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUtx1ZQkmlTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "afef6c71-d571-497e-9b8b-239a9b852bb7"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset= dataset.window(5, shift=1, drop_remainder= True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "dataset= dataset.shuffle(buffer_size=10)\n",
        "for x,y in dataset:\n",
        "  print(x.numpy(), y.numpy())\n",
        "\n",
        "## Note: shuffling didn't occur in the element of a window instead it shuffled the window in the dataset."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4] [5]\n",
            "[3 4 5 6] [7]\n",
            "[4 5 6 7] [8]\n",
            "[5 6 7 8] [9]\n",
            "[2 3 4 5] [6]\n",
            "[0 1 2 3] [4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7ExaShY5hmh",
        "colab_type": "text"
      },
      "source": [
        "> We then use the batch method to batches of 2 windows at each training iterations amd also called a prefetch method which will ensure that `Tensorflow` will load the nect batch of data while it's working on teh current batch of data.\n",
        "\n",
        "> \"This is done, so that it never runs out of data and the GPU is kept busy as much as possible.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqv6_vtB4_B-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7e1e4a42-5c27-4160-a4b1-4794a5336b79"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset= dataset.window(5, shift=1, drop_remainder= True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "dataset= dataset.shuffle(buffer_size=10)\n",
        "dataset= dataset.batch(batch_size=2).prefetch(1)\n",
        "for x,y in dataset:\n",
        "  print(\"X= \",x.numpy())\n",
        "  print(\"Y= \",y.numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X=  [[4 5 6 7]\n",
            " [2 3 4 5]]\n",
            "Y=  [[8]\n",
            " [6]]\n",
            "X=  [[3 4 5 6]\n",
            " [0 1 2 3]]\n",
            "Y=  [[7]\n",
            " [4]]\n",
            "X=  [[5 6 7 8]\n",
            " [1 2 3 4]]\n",
            "Y=  [[9]\n",
            " [5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "higKTtZe67G7",
        "colab_type": "text"
      },
      "source": [
        " Now will wrap every thing in a function by giving it a time series, a window size, the batch size which defaults to 32, a shuffle buffer which defaults to 1000 and then just run it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUPhhom46xdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def window_dataset(series, window_size, batch_size=32, shuffle_buffer=1000):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(window_size+1, shift+1,drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_siz+1))\n",
        "  dataset = dataset.shuffle(shuffle_buffer)\n",
        "  dataset= dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "  dataset= dataset.batch(batch_size).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDJ_jlCh9oRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}